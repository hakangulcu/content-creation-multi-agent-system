# Content Creation Multi-Agent System Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================

# Ollama Model Configuration
# Popular models: llama3.1:8b, llama3.1:70b, mistral:7b, codellama:7b, phi3:mini
OLLAMA_MODEL=llama3.1:8b

# Ollama Server Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Model Parameters (Optional - defaults are set in code)
OLLAMA_TEMPERATURE=0.7
OLLAMA_TOP_P=0.9
OLLAMA_TOP_K=40
OLLAMA_REPEAT_PENALTY=1.1
OLLAMA_NUM_PREDICT=4096

# =============================================================================
# OPTIONAL API KEYS (for enhanced web search)
# =============================================================================

# Google Search API (Optional - for enhanced web search)
# Get from: https://developers.google.com/custom-search/v1/introduction
GOOGLE_SEARCH_API_KEY=your_google_search_api_key_here
GOOGLE_SEARCH_ENGINE_ID=your_custom_search_engine_id_here

# Serp API (Alternative search provider)
SERP_API_KEY=your_serp_api_key_here

# =============================================================================
# CONFIGURATION SETTINGS
# =============================================================================

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=logs/content_creation.log

# Content Generation Settings
DEFAULT_WORD_COUNT=1500
DEFAULT_TONE=professional
MAX_RESEARCH_SOURCES=10
MAX_RETRY_ATTEMPTS=3

# File Output Settings
OUTPUT_DIRECTORY=outputs
BACKUP_DIRECTORY=backups

# Performance Settings
REQUEST_TIMEOUT=300
CONCURRENT_REQUESTS=2  # Reduced for local models
RATE_LIMIT_DELAY=2     # Increased for local processing

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Development Mode (set to true for verbose logging and debugging)
DEBUG_MODE=false

# Test Configuration
TEST_MODEL=llama3.1:8b
MOCK_RESPONSES=false

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# Local Model Rate Limiting
MAX_REQUESTS_PER_MINUTE=30  # Reduced for local processing
MAX_TOKENS_PER_HOUR=50000   # Reduced for local processing

# Content Filtering
ENABLE_CONTENT_FILTER=true
PROFANITY_FILTER=true

# =============================================================================
# OLLAMA SETUP INSTRUCTIONS
# =============================================================================

# 1. Install Ollama:
#    - Linux/macOS: curl -fsSL https://ollama.com/install.sh | sh
#    - Windows: Download from https://ollama.com/download/windows
#    - Or use Docker: docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# 2. Start Ollama server:
#    ollama serve

# 3. Pull your preferred model:
#    ollama pull llama3.1:8b      # Good balance of quality and speed
#    ollama pull llama3.1:70b     # High quality (requires more resources)
#    ollama pull mistral:7b       # Fast and efficient
#    ollama pull phi3:mini        # Very fast, smaller model

# 4. Test the setup:
#    ollama run llama3.1:8b "Hello, how are you?"

# =============================================================================
# RECOMMENDED MODELS FOR CONTENT CREATION
# =============================================================================

# BALANCED (Recommended for most users):
# OLLAMA_MODEL=llama3.1:8b
# - Good quality content generation
# - Reasonable speed
# - Moderate resource usage (8GB RAM recommended)

# HIGH QUALITY (For best results):
# OLLAMA_MODEL=llama3.1:70b
# - Excellent content quality
# - Slower generation
# - High resource usage (64GB+ RAM recommended)

# FAST (For quick testing):
# OLLAMA_MODEL=phi3:mini
# - Fast generation
# - Lower quality
# - Low resource usage (4GB RAM sufficient)

# CODING FOCUSED (For technical content):
# OLLAMA_MODEL=codellama:7b
# - Good for technical articles
# - Code examples and explanations
# - Moderate resource usage

# =============================================================================
# MINIMUM REQUIREMENTS
# =============================================================================

# Hardware Requirements (for llama3.1:8b):
# - RAM: 8GB minimum, 16GB recommended
# - Storage: 5GB for model + 2GB for dependencies
# - CPU: Modern multi-core processor
# - GPU: Optional but recommended for faster inference

# Network Requirements:
# - Initial setup: Internet connection to download models
# - Runtime: No internet required (fully local)

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# Common Issues:
# 1. "Connection refused" - Make sure Ollama is running: ollama serve
# 2. "Model not found" - Pull the model: ollama pull llama3.1:8b
# 3. "Out of memory" - Try a smaller model like phi3:mini
# 4. Slow generation - Reduce max_tokens or try GPU acceleration

# Performance Tips:
# - Use GPU if available: Install NVIDIA drivers and CUDA
# - Increase RAM for better performance
# - Use SSD storage for faster model loading
# - Close other applications to free resources

# =============================================================================
# EXAMPLE MINIMAL CONFIGURATION
# =============================================================================

# To get started quickly, you only need:
# OLLAMA_MODEL=llama3.1:8b
# OLLAMA_BASE_URL=http://localhost:11434